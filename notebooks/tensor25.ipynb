{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensor25.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhpdE5u0CBbMov7ZEuwq19",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhoujiuzhou9/tensorFlow/blob/V1/tensor25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "hmgdU14YjMGt",
        "outputId": "ec8e658e-3d2c-4a8c-c145-43cd3e29d453"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6529dde2ed93>\"\u001b[0;36m, line \u001b[0;32m128\u001b[0m\n\u001b[0;31m    tf.summary.image('val-images:', plot_to_image(figure), step=step)import numpy as np\u001b[0m\n\u001b[0m                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import io\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
        "    y = tf.cast(y, dtype=tf.int32)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def plot_to_image(figure):\n",
        "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "    # Save the plot to a PNG in memory.\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    # Closing the figure prevents it from being displayed directly inside\n",
        "    # the notebook.\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    # Convert PNG buffer to TF image\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    # Add the batch dimension\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_grid(images):\n",
        "    \"\"\"Return a 5x5 grid of the MNIST images as a matplotlib figure.\"\"\"\n",
        "    # Create a figure to contain the plot.\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    for i in range(25):\n",
        "        # Start next subplot.\n",
        "        plt.subplot(5, 5, i + 1, title='name')\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "\n",
        "    return figure\n",
        "\n",
        "\n",
        "batchsz = 128\n",
        "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
        "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
        "\n",
        "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "db = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n",
        "\n",
        "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "ds_val = ds_val.map(preprocess).batch(batchsz, drop_remainder=True)\n",
        "\n",
        "network = Sequential([layers.Dense(256, activation='relu'),\n",
        "                      layers.Dense(128, activation='relu'),\n",
        "                      layers.Dense(64, activation='relu'),\n",
        "                      layers.Dense(32, activation='relu'),\n",
        "                      layers.Dense(10)])\n",
        "network.build(input_shape=(None, 28 * 28))\n",
        "network.summary()\n",
        "\n",
        "optimizer = optimizers.Adam(lr=0.01)\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = 'logs/' + current_time\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "# get x from (x,y)\n",
        "sample_img = next(iter(db))[0]\n",
        "# get first image instance\n",
        "sample_img = sample_img[0]\n",
        "sample_img = tf.reshape(sample_img, [1, 28, 28, 1])\n",
        "with summary_writer.as_default():\n",
        "    tf.summary.image(\"Training sample:\", sample_img, step=0)\n",
        "\n",
        "for step, (x, y) in enumerate(db):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # [b, 28, 28] => [b, 784]\n",
        "        x = tf.reshape(x, (-1, 28 * 28))\n",
        "        # [b, 784] => [b, 10]\n",
        "        out = network(x)\n",
        "        # [b] => [b, 10]\n",
        "        y_onehot = tf.one_hot(y, depth=10)\n",
        "        # [b]\n",
        "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n",
        "\n",
        "    grads = tape.gradient(loss, network.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, 'loss:', float(loss))\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar('train-loss', float(loss), step=step)\n",
        "\n",
        "            # evaluate\n",
        "    if step % 500 == 0:\n",
        "        total, total_correct = 0., 0\n",
        "\n",
        "        for _, (x, y) in enumerate(ds_val):\n",
        "            # [b, 28, 28] => [b, 784]\n",
        "            x = tf.reshape(x, (-1, 28 * 28))\n",
        "            # [b, 784] => [b, 10]\n",
        "            out = network(x)\n",
        "            # [b, 10] => [b]\n",
        "            pred = tf.argmax(out, axis=1)\n",
        "            pred = tf.cast(pred, dtype=tf.int32)\n",
        "            # bool type\n",
        "            correct = tf.equal(pred, y)\n",
        "            # bool tensor => int tensor => numpy\n",
        "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
        "            total += x.shape[0]\n",
        "\n",
        "        print(step, 'Evaluate Acc:', total_correct / total)\n",
        "\n",
        "        # print(x.shape)\n",
        "        val_images = x[:25]\n",
        "        val_images = tf.reshape(val_images, [-1, 28, 28, 1])\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar('test-acc', float(total_correct / total), step=step)\n",
        "            tf.summary.image(\"val-onebyone-images:\", val_images, max_outputs=25, step=step)\n",
        "\n",
        "            val_images = tf.reshape(val_images, [-1, 28, 28])\n",
        "            figure = image_grid(val_images)\n",
        "            tf.summary.image('val-images:', plot_to_image(figure), step=step)import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def himmelblau(x):\n",
        "    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
        "\n",
        "\n",
        "x = np.arange(-6, 6, 0.1)\n",
        "y = np.arange(-6, 6, 0.1)\n",
        "print('x,y range:', x.shape, y.shape)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "print('X,Y maps:', X.shape, Y.shape)\n",
        "Z = himmelblau([X, Y])\n",
        "\n",
        "fig = plt.figure('himmelblau')\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.plot_surface(X, Y, Z)\n",
        "ax.view_init(60, -30)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.show()\n",
        "\n",
        "# [1., 0.], [-4, 0.], [4, 0.]\n",
        "x = tf.constant([4., 0.])\n",
        "\n",
        "for step in range(2000):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch([x])\n",
        "        y = himmelblau(x)\n",
        "\n",
        "    grads = tape.gradient(y, [x])[0]\n",
        "    x -= 0.01 * grads\n",
        "\n",
        "    if step % 20 == 0:\n",
        "        print('step {}: x = {}, f(x) = {}'\n",
        "              .format(step, x.numpy(), y.numpy()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
        "    y = tf.cast(y, dtype=tf.int32)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "(x, y), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "batchsz = 128\n",
        "\n",
        "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "db = db.map(preprocess).shuffle(10000).batch(batchsz)\n",
        "\n",
        "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "db_test = db_test.map(preprocess).batch(batchsz)\n",
        "\n",
        "db_iter = iter(db)\n",
        "sample = next(db_iter)\n",
        "print('batch:', sample[0].shape, sample[1].shape)\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Dense(256, activation=tf.nn.relu),  # [b, 784] => [b, 256]\n",
        "    layers.Dense(128, activation=tf.nn.relu),  # [b, 256] => [b, 128]\n",
        "    layers.Dense(64, activation=tf.nn.relu),  # [b, 128] => [b, 64]\n",
        "    layers.Dense(32, activation=tf.nn.relu),  # [b, 64] => [b, 32]\n",
        "    layers.Dense(10)  # [b, 32] => [b, 10], 330 = 32*10 + 10\n",
        "])\n",
        "model.build(input_shape=[None, 28 * 28])\n",
        "model.summary()\n",
        "# w = w - lr*grad\n",
        "optimizer = optimizers.Adam(lr=1e-3)\n",
        "\n",
        "\n",
        "def main():\n",
        "    for epoch in range(30):\n",
        "\n",
        "        for step, (x, y) in enumerate(db):\n",
        "\n",
        "            # x: [b, 28, 28] => [b, 784]\n",
        "            # y: [b]\n",
        "            x = tf.reshape(x, [-1, 28 * 28])\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # [b, 784] => [b, 10]\n",
        "                logits = model(x)\n",
        "                y_onehot = tf.one_hot(y, depth=10)\n",
        "                # [b]\n",
        "                loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n",
        "                loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
        "                loss_ce = tf.reduce_mean(loss_ce)\n",
        "\n",
        "            grads = tape.gradient(loss_ce, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                print(epoch, step, 'loss:', float(loss_ce), float(loss_mse))\n",
        "\n",
        "        # test\n",
        "        total_correct = 0\n",
        "        total_num = 0\n",
        "        for x, y in db_test:\n",
        "            # x: [b, 28, 28] => [b, 784]\n",
        "            # y: [b]\n",
        "            x = tf.reshape(x, [-1, 28 * 28])\n",
        "            # [b, 10]\n",
        "            logits = model(x)\n",
        "            # logits => prob, [b, 10]\n",
        "            prob = tf.nn.softmax(logits, axis=1)\n",
        "            # [b, 10] => [b], int64\n",
        "            pred = tf.argmax(prob, axis=1)\n",
        "            pred = tf.cast(pred, dtype=tf.int32)\n",
        "            # pred:[b]\n",
        "            # y: [b]\n",
        "            # correct: [b], True: equal, False: not equal\n",
        "            correct = tf.equal(pred, y)\n",
        "            correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
        "\n",
        "            total_correct += int(correct)\n",
        "            total_num += x.shape[0]\n",
        "\n",
        "        acc = total_correct / total_num\n",
        "        print(epoch, 'test acc:', acc)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClG1ZymAj8tj",
        "outputId": "2e6da481-fae5-41ce-8c78-7806ed91afaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,)\n",
            "batch: (128, 28, 28) (128,)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 244,522\n",
            "Trainable params: 244,522\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 loss: 2.3036022186279297 0.13759845495224\n",
            "0 100 loss: 0.6683329343795776 16.608823776245117\n",
            "0 200 loss: 0.5584144592285156 22.209192276000977\n",
            "0 300 loss: 0.4342808723449707 21.0661563873291\n",
            "0 400 loss: 0.5357062816619873 23.7303466796875\n",
            "0 test acc: 0.8308\n",
            "1 0 loss: 0.40553343296051025 22.592790603637695\n",
            "1 100 loss: 0.3611891567707062 27.45064926147461\n",
            "1 200 loss: 0.27773529291152954 24.951749801635742\n",
            "1 300 loss: 0.5316498279571533 23.497722625732422\n",
            "1 400 loss: 0.43627214431762695 26.46511459350586\n",
            "1 test acc: 0.8642\n",
            "2 0 loss: 0.3477035164833069 23.976612091064453\n",
            "2 100 loss: 0.42920762300491333 29.51603126525879\n",
            "2 200 loss: 0.23426952958106995 23.233659744262695\n",
            "2 300 loss: 0.278328537940979 27.882373809814453\n",
            "2 400 loss: 0.25249800086021423 28.196243286132812\n",
            "2 test acc: 0.867\n",
            "3 0 loss: 0.3500705659389496 23.780540466308594\n",
            "3 100 loss: 0.2528729736804962 30.52159309387207\n",
            "3 200 loss: 0.25393760204315186 29.07466697692871\n",
            "3 300 loss: 0.2500509023666382 32.60387420654297\n",
            "3 400 loss: 0.29635077714920044 31.25417137145996\n",
            "3 test acc: 0.8668\n",
            "4 0 loss: 0.3578717112541199 34.7981071472168\n",
            "4 100 loss: 0.29755347967147827 31.762680053710938\n",
            "4 200 loss: 0.2383497804403305 35.18058776855469\n",
            "4 300 loss: 0.11656571924686432 35.343910217285156\n",
            "4 400 loss: 0.3450804352760315 34.684120178222656\n",
            "4 test acc: 0.8759\n",
            "5 0 loss: 0.24787302315235138 39.9080810546875\n",
            "5 100 loss: 0.21010147035121918 41.953922271728516\n",
            "5 200 loss: 0.3221758008003235 46.865718841552734\n",
            "5 300 loss: 0.21811208128929138 49.84811782836914\n",
            "5 400 loss: 0.3137308955192566 43.454933166503906\n",
            "5 test acc: 0.8766\n",
            "6 0 loss: 0.2450864017009735 42.851863861083984\n",
            "6 100 loss: 0.20750081539154053 43.67017364501953\n",
            "6 200 loss: 0.24372288584709167 40.31488037109375\n",
            "6 300 loss: 0.1983211636543274 47.21202087402344\n",
            "6 400 loss: 0.2859076261520386 55.308135986328125\n",
            "6 test acc: 0.8812\n",
            "7 0 loss: 0.23174187541007996 47.95658874511719\n",
            "7 100 loss: 0.2069035768508911 46.914939880371094\n",
            "7 200 loss: 0.258141428232193 43.168701171875\n",
            "7 300 loss: 0.3207840323448181 42.946224212646484\n",
            "7 400 loss: 0.20446869730949402 52.90155029296875\n",
            "7 test acc: 0.8786\n",
            "8 0 loss: 0.2820696234703064 52.050201416015625\n",
            "8 100 loss: 0.32149720191955566 65.10173797607422\n",
            "8 200 loss: 0.23648755252361298 52.58378982543945\n",
            "8 300 loss: 0.2611667513847351 53.6528434753418\n",
            "8 400 loss: 0.20858487486839294 57.35271072387695\n",
            "8 test acc: 0.8842\n",
            "9 0 loss: 0.29099804162979126 53.15629577636719\n",
            "9 100 loss: 0.3075891435146332 60.953834533691406\n",
            "9 200 loss: 0.1668279618024826 68.74314880371094\n",
            "9 300 loss: 0.20825955271720886 63.27125549316406\n",
            "9 400 loss: 0.3072701692581177 50.446685791015625\n",
            "9 test acc: 0.8877\n",
            "10 0 loss: 0.28173625469207764 75.50052642822266\n",
            "10 100 loss: 0.21087554097175598 65.28877258300781\n",
            "10 200 loss: 0.3025035858154297 72.462890625\n",
            "10 300 loss: 0.18408939242362976 62.856475830078125\n",
            "10 400 loss: 0.27746641635894775 59.04938507080078\n",
            "10 test acc: 0.88\n",
            "11 0 loss: 0.14270827174186707 65.48206329345703\n",
            "11 100 loss: 0.2877749800682068 80.08861541748047\n",
            "11 200 loss: 0.323627233505249 61.714534759521484\n",
            "11 300 loss: 0.22474554181098938 82.95988464355469\n",
            "11 400 loss: 0.17896872758865356 57.808387756347656\n",
            "11 test acc: 0.8844\n",
            "12 0 loss: 0.1984173059463501 70.82591247558594\n",
            "12 100 loss: 0.187097430229187 66.67967987060547\n",
            "12 200 loss: 0.15928861498832703 90.92371368408203\n",
            "12 300 loss: 0.18546217679977417 72.75948333740234\n",
            "12 400 loss: 0.23753133416175842 69.45862579345703\n",
            "12 test acc: 0.8652\n",
            "13 0 loss: 0.23447515070438385 97.56390380859375\n",
            "13 100 loss: 0.25590789318084717 82.60414123535156\n",
            "13 200 loss: 0.18193110823631287 71.96871948242188\n",
            "13 300 loss: 0.2271900475025177 62.940120697021484\n",
            "13 400 loss: 0.1311003863811493 88.26107788085938\n",
            "13 test acc: 0.8831\n",
            "14 0 loss: 0.15930010378360748 81.1951675415039\n",
            "14 100 loss: 0.2238391637802124 86.09869384765625\n",
            "14 200 loss: 0.11237286031246185 106.79399108886719\n",
            "14 300 loss: 0.15025952458381653 102.97172546386719\n",
            "14 400 loss: 0.23952609300613403 82.51780700683594\n",
            "14 test acc: 0.8857\n",
            "15 0 loss: 0.27160003781318665 85.99639892578125\n",
            "15 100 loss: 0.12819108366966248 91.83995819091797\n",
            "15 200 loss: 0.24678410589694977 81.27045440673828\n",
            "15 300 loss: 0.16544851660728455 106.75804138183594\n",
            "15 400 loss: 0.21365505456924438 103.74758911132812\n",
            "15 test acc: 0.8912\n",
            "16 0 loss: 0.20117250084877014 78.97911071777344\n",
            "16 100 loss: 0.15949612855911255 107.9219970703125\n",
            "16 200 loss: 0.17540885508060455 97.7059326171875\n",
            "16 300 loss: 0.27829456329345703 81.38008117675781\n",
            "16 400 loss: 0.17612531781196594 93.9190673828125\n",
            "16 test acc: 0.8899\n",
            "17 0 loss: 0.3110872507095337 78.78697204589844\n",
            "17 100 loss: 0.1771744191646576 108.63336944580078\n",
            "17 200 loss: 0.09767572581768036 101.32174682617188\n",
            "17 300 loss: 0.1255965381860733 99.42489624023438\n",
            "17 400 loss: 0.1727128028869629 107.62004089355469\n",
            "17 test acc: 0.8835\n",
            "18 0 loss: 0.12873929738998413 88.30271911621094\n",
            "18 100 loss: 0.15503205358982086 96.24614715576172\n",
            "18 200 loss: 0.20002800226211548 112.8110122680664\n",
            "18 300 loss: 0.17779703438282013 113.13794708251953\n",
            "18 400 loss: 0.213194340467453 113.84833526611328\n",
            "18 test acc: 0.893\n",
            "19 0 loss: 0.15799273550510406 118.25938415527344\n",
            "19 100 loss: 0.14764843881130219 137.7627410888672\n",
            "19 200 loss: 0.22275078296661377 94.24462127685547\n",
            "19 300 loss: 0.1597558706998825 106.25508117675781\n",
            "19 400 loss: 0.16937732696533203 105.83148193359375\n",
            "19 test acc: 0.8888\n",
            "20 0 loss: 0.16040967404842377 91.89527893066406\n",
            "20 100 loss: 0.1293913871049881 135.35586547851562\n",
            "20 200 loss: 0.19991512596607208 144.17893981933594\n",
            "20 300 loss: 0.18091514706611633 89.49383544921875\n",
            "20 400 loss: 0.1231788843870163 136.27938842773438\n",
            "20 test acc: 0.8893\n",
            "21 0 loss: 0.1515234112739563 127.80339813232422\n",
            "21 100 loss: 0.14520354568958282 122.68846130371094\n",
            "21 200 loss: 0.15645334124565125 104.92207336425781\n",
            "21 300 loss: 0.08851955831050873 132.94871520996094\n",
            "21 400 loss: 0.17554567754268646 148.59483337402344\n",
            "21 test acc: 0.8826\n",
            "22 0 loss: 0.1329168677330017 117.22853088378906\n",
            "22 100 loss: 0.1407097429037094 147.54075622558594\n",
            "22 200 loss: 0.10985000431537628 117.68692016601562\n",
            "22 300 loss: 0.17756317555904388 147.00506591796875\n",
            "22 400 loss: 0.10364723950624466 143.65255737304688\n",
            "22 test acc: 0.8947\n",
            "23 0 loss: 0.17289382219314575 130.2093505859375\n",
            "23 100 loss: 0.1314655840396881 117.9459228515625\n",
            "23 200 loss: 0.13510005176067352 166.51760864257812\n",
            "23 300 loss: 0.2227037250995636 176.4247283935547\n",
            "23 400 loss: 0.16080139577388763 140.33303833007812\n",
            "23 test acc: 0.8938\n",
            "24 0 loss: 0.1434686779975891 111.26937103271484\n",
            "24 100 loss: 0.10027579963207245 212.0259246826172\n",
            "24 200 loss: 0.11541455239057541 140.2072296142578\n",
            "24 300 loss: 0.11272058635950089 199.61875915527344\n",
            "24 400 loss: 0.1778540015220642 193.77294921875\n",
            "24 test acc: 0.8936\n",
            "25 0 loss: 0.11223744601011276 140.9910430908203\n",
            "25 100 loss: 0.1238367110490799 206.77928161621094\n",
            "25 200 loss: 0.2328440248966217 189.65980529785156\n",
            "25 300 loss: 0.2052101343870163 156.1820068359375\n",
            "25 400 loss: 0.20924869179725647 171.70132446289062\n",
            "25 test acc: 0.8914\n",
            "26 0 loss: 0.15296484529972076 181.84475708007812\n",
            "26 100 loss: 0.13295936584472656 128.2520751953125\n",
            "26 200 loss: 0.18691284954547882 202.20497131347656\n",
            "26 300 loss: 0.09497541189193726 190.345947265625\n",
            "26 400 loss: 0.10642726719379425 152.06141662597656\n",
            "26 test acc: 0.8926\n",
            "27 0 loss: 0.1801399141550064 154.49111938476562\n",
            "27 100 loss: 0.13578617572784424 200.88047790527344\n",
            "27 200 loss: 0.07833077013492584 184.087158203125\n",
            "27 300 loss: 0.15398424863815308 160.8055419921875\n",
            "27 400 loss: 0.12236377596855164 157.28662109375\n",
            "27 test acc: 0.8954\n",
            "28 0 loss: 0.09496258944272995 171.16665649414062\n",
            "28 100 loss: 0.10572335869073868 156.79156494140625\n",
            "28 200 loss: 0.07627731561660767 209.98236083984375\n",
            "28 300 loss: 0.14298462867736816 175.22457885742188\n",
            "28 400 loss: 0.10292273759841919 198.2884521484375\n",
            "28 test acc: 0.8951\n",
            "29 0 loss: 0.09260556101799011 182.29428100585938\n",
            "29 100 loss: 0.09149859845638275 193.21646118164062\n",
            "29 200 loss: 0.13211727142333984 184.24000549316406\n",
            "29 300 loss: 0.14286617934703827 164.11575317382812\n",
            "29 400 loss: 0.14164580404758453 135.2999725341797\n",
            "29 test acc: 0.8895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import io\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
        "    y = tf.cast(y, dtype=tf.int32)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def plot_to_image(figure):\n",
        "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "    # Save the plot to a PNG in memory.\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    # Closing the figure prevents it from being displayed directly inside\n",
        "    # the notebook.\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    # Convert PNG buffer to TF image\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    # Add the batch dimension\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_grid(images):\n",
        "    \"\"\"Return a 5x5 grid of the MNIST images as a matplotlib figure.\"\"\"\n",
        "    # Create a figure to contain the plot.\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    for i in range(25):\n",
        "        # Start next subplot.\n",
        "        plt.subplot(5, 5, i + 1, title='name')\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "\n",
        "    return figure\n",
        "\n",
        "\n",
        "batchsz = 128\n",
        "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
        "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
        "\n",
        "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "db = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n",
        "\n",
        "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "ds_val = ds_val.map(preprocess).batch(batchsz, drop_remainder=True)\n",
        "\n",
        "network = Sequential([layers.Dense(256, activation='relu'),\n",
        "                      layers.Dense(128, activation='relu'),\n",
        "                      layers.Dense(64, activation='relu'),\n",
        "                      layers.Dense(32, activation='relu'),\n",
        "                      layers.Dense(10)])\n",
        "network.build(input_shape=(None, 28 * 28))\n",
        "network.summary()\n",
        "\n",
        "optimizer = optimizers.Adam(lr=0.01)\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = 'logs/' + current_time\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "# get x from (x,y)\n",
        "sample_img = next(iter(db))[0]\n",
        "# get first image instance\n",
        "sample_img = sample_img[0]\n",
        "sample_img = tf.reshape(sample_img, [1, 28, 28, 1])\n",
        "with summary_writer.as_default():\n",
        "    tf.summary.image(\"Training sample:\", sample_img, step=0)\n",
        "\n",
        "for step, (x, y) in enumerate(db):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # [b, 28, 28] => [b, 784]\n",
        "        x = tf.reshape(x, (-1, 28 * 28))\n",
        "        # [b, 784] => [b, 10]\n",
        "        out = network(x)\n",
        "        # [b] => [b, 10]\n",
        "        y_onehot = tf.one_hot(y, depth=10)\n",
        "        # [b]\n",
        "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n",
        "\n",
        "    grads = tape.gradient(loss, network.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, 'loss:', float(loss))\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar('train-loss', float(loss), step=step)\n",
        "\n",
        "            # evaluate\n",
        "    if step % 500 == 0:\n",
        "        total, total_correct = 0., 0\n",
        "\n",
        "        for _, (x, y) in enumerate(ds_val):\n",
        "            # [b, 28, 28] => [b, 784]\n",
        "            x = tf.reshape(x, (-1, 28 * 28))\n",
        "            # [b, 784] => [b, 10]\n",
        "            out = network(x)\n",
        "            # [b, 10] => [b]\n",
        "            pred = tf.argmax(out, axis=1)\n",
        "            pred = tf.cast(pred, dtype=tf.int32)\n",
        "            # bool type\n",
        "            correct = tf.equal(pred, y)\n",
        "            # bool tensor => int tensor => numpy\n",
        "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
        "            total += x.shape[0]\n",
        "\n",
        "        print(step, 'Evaluate Acc:', total_correct / total)\n",
        "\n",
        "        # print(x.shape)\n",
        "        val_images = x[:25]\n",
        "        val_images = tf.reshape(val_images, [-1, 28, 28, 1])\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar('test-acc', float(total_correct / total), step=step)\n",
        "            tf.summary.image(\"val-onebyone-images:\", val_images, max_outputs=25, step=step)\n",
        "\n",
        "            val_images = tf.reshape(val_images, [-1, 28, 28])\n",
        "            figure = image_grid(val_images)\n",
        "            tf.summary.image('val-images:', plot_to_image(figure), step=step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfPIcjyDt322",
        "outputId": "3a9effcf-2a56-4634-f9b5-9c4b86225684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "datasets: (60000, 28, 28) (60000,) 0 255\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 244,522\n",
            "Trainable params: 244,522\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss: 2.290738344192505\n",
            "0 Evaluate Acc: 0.19180689102564102\n",
            "100 loss: 0.27134042978286743\n",
            "200 loss: 0.1590808779001236\n",
            "300 loss: 0.08030091971158981\n",
            "400 loss: 0.20215734839439392\n",
            "500 loss: 0.250488817691803\n",
            "500 Evaluate Acc: 0.9631410256410257\n",
            "600 loss: 0.14117342233657837\n",
            "700 loss: 0.08998619019985199\n",
            "800 loss: 0.10063408315181732\n",
            "900 loss: 0.08003776520490646\n",
            "1000 loss: 0.07922761887311935\n",
            "1000 Evaluate Acc: 0.9663461538461539\n",
            "1100 loss: 0.09494095295667648\n",
            "1200 loss: 0.033312804996967316\n",
            "1300 loss: 0.13337430357933044\n",
            "1400 loss: 0.06263656914234161\n",
            "1500 loss: 0.138874351978302\n",
            "1500 Evaluate Acc: 0.9708533653846154\n",
            "1600 loss: 0.05331477150321007\n",
            "1700 loss: 0.13126152753829956\n",
            "1800 loss: 0.17166435718536377\n",
            "1900 loss: 0.08976654708385468\n",
            "2000 loss: 0.07110652327537537\n",
            "2000 Evaluate Acc: 0.9682491987179487\n",
            "2100 loss: 0.011408806778490543\n",
            "2200 loss: 0.025039955973625183\n",
            "2300 loss: 0.08826176822185516\n",
            "2400 loss: 0.02895274944603443\n",
            "2500 loss: 0.16170324385166168\n",
            "2500 Evaluate Acc: 0.9722556089743589\n",
            "2600 loss: 0.24716193974018097\n",
            "2700 loss: 0.1157083809375763\n",
            "2800 loss: 0.03247763589024544\n",
            "2900 loss: 0.06416794657707214\n",
            "3000 loss: 0.05755723640322685\n",
            "3000 Evaluate Acc: 0.9692508012820513\n",
            "3100 loss: 0.04932260513305664\n",
            "3200 loss: 0.13357962667942047\n",
            "3300 loss: 0.0823112428188324\n",
            "3400 loss: 0.05913912132382393\n",
            "3500 loss: 0.07650849223136902\n",
            "3500 Evaluate Acc: 0.9769631410256411\n",
            "3600 loss: 0.02154359221458435\n",
            "3700 loss: 0.010867485776543617\n",
            "3800 loss: 0.07251040637493134\n",
            "3900 loss: 0.10561271011829376\n",
            "4000 loss: 0.09347788989543915\n",
            "4000 Evaluate Acc: 0.9709535256410257\n",
            "4100 loss: 0.010588983073830605\n",
            "4200 loss: 0.35542333126068115\n",
            "4300 loss: 0.03448385000228882\n",
            "4400 loss: 0.007472609169781208\n",
            "4500 loss: 0.03530357405543327\n",
            "4500 Evaluate Acc: 0.9732572115384616\n",
            "4600 loss: 0.11851414293050766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "    x is a simple image, not a batch\n",
        "    \"\"\"\n",
        "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
        "    x = tf.reshape(x, [28 * 28])\n",
        "    y = tf.cast(y, dtype=tf.int32)\n",
        "    y = tf.one_hot(y, depth=10)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "batchsz = 128\n",
        "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
        "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
        "\n",
        "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "db = db.map(preprocess).shuffle(60000).batch(batchsz)\n",
        "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "ds_val = ds_val.map(preprocess).batch(batchsz)\n",
        "\n",
        "sample = next(iter(db))\n",
        "print(sample[0].shape, sample[1].shape)\n",
        "\n",
        "network = Sequential([layers.Dense(256, activation='relu'),\n",
        "                      layers.Dense(128, activation='relu'),\n",
        "                      layers.Dense(64, activation='relu'),\n",
        "                      layers.Dense(32, activation='relu'),\n",
        "                      layers.Dense(10)])\n",
        "network.build(input_shape=(None, 28 * 28))\n",
        "network.summary()\n",
        "\n",
        "\n",
        "class MyDense(layers.Layer):\n",
        "\n",
        "    def __init__(self, inp_dim, outp_dim):\n",
        "        super(MyDense, self).__init__()\n",
        "\n",
        "        self.kernel = self.add_variable('w', [inp_dim, outp_dim])\n",
        "        self.bias = self.add_variable('b', [outp_dim])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        out = inputs @ self.kernel + self.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.fc1 = MyDense(28 * 28, 256)\n",
        "        self.fc2 = MyDense(256, 128)\n",
        "        self.fc3 = MyDense(128, 64)\n",
        "        self.fc4 = MyDense(64, 32)\n",
        "        self.fc5 = MyDense(32, 10)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = self.fc1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.fc5(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "network = MyModel()\n",
        "\n",
        "network.compile(optimizer=optimizers.Adam(lr=0.01),\n",
        "                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "network.fit(db, epochs=5, validation_data=ds_val,\n",
        "            validation_freq=2)\n",
        "\n",
        "network.evaluate(ds_val)\n",
        "\n",
        "sample = next(iter(ds_val))\n",
        "x = sample[0]\n",
        "y = sample[1]  # one-hot\n",
        "pred = network.predict(x)  # [b, 10]\n",
        "# convert back to number \n",
        "y = tf.argmax(y, axis=1)\n",
        "pred = tf.argmax(pred, axis=1)\n",
        "\n",
        "print(pred)\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEtc_0DUh2f4",
        "outputId": "f440e53f-b6c7-49a0-dbe6-7383c943f20a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "datasets: (60000, 28, 28) (60000,) 0 255\n",
            "(128, 784) (128, 10)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 244,522\n",
            "Trainable params: 244,522\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 10s 5ms/step - loss: 0.2814 - accuracy: 0.9143\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 4s 4ms/step - loss: 0.1398 - accuracy: 0.9612 - val_loss: 0.1624 - val_accuracy: 0.9560\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 4ms/step - loss: 0.1098 - accuracy: 0.9699\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 4ms/step - loss: 0.1027 - accuracy: 0.9725 - val_loss: 0.1497 - val_accuracy: 0.9617\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 3s 3ms/step - loss: 0.0833 - accuracy: 0.9785\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9688\n",
            "tf.Tensor(\n",
            "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7\n",
            " 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 9 6 4 3 0 7 0 2 9\n",
            " 1 7 3 2 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8\n",
            " 7 3 9 7 4 4 4 9 2 5 4 7 6 9 9 0 5], shape=(128,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7\n",
            " 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9\n",
            " 1 7 3 2 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8\n",
            " 7 3 9 7 4 4 4 9 2 5 4 7 6 7 9 0 5], shape=(128,), dtype=int64)\n"
          ]
        }
      ]
    }
  ]
}