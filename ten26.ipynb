{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKOkgjEP5j55vNGmXdtYCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhoujiuzhou9/tensorFlow/blob/V1/ten26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLJt2xVaBj3O",
        "outputId": "e0530903-cac6-48ed-8424-46ff5d2468be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 6s 0us/step\n",
            "169017344/169001437 [==============================] - 6s 0us/step\n",
            "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)\n",
            "sample: (128, 32, 32, 3) (128,) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 loss: 4.605703353881836\n",
            "0 100 loss: 4.600923538208008\n",
            "0 200 loss: 4.362790107727051\n",
            "0 300 loss: 4.333405494689941\n",
            "0 acc: 0.0593\n",
            "1 0 loss: 4.070964813232422\n",
            "1 100 loss: 4.076962471008301\n",
            "1 200 loss: 3.8302462100982666\n",
            "1 300 loss: 4.006497383117676\n",
            "1 acc: 0.1165\n",
            "2 0 loss: 3.847036838531494\n",
            "2 100 loss: 3.8636465072631836\n",
            "2 200 loss: 3.638575553894043\n",
            "2 300 loss: 3.517000198364258\n",
            "2 acc: 0.1604\n",
            "3 0 loss: 3.569208860397339\n",
            "3 100 loss: 3.553874969482422\n",
            "3 200 loss: 3.1499390602111816\n",
            "3 300 loss: 3.1406352519989014\n",
            "3 acc: 0.2058\n",
            "4 0 loss: 3.457406759262085\n",
            "4 100 loss: 3.353813648223877\n",
            "4 200 loss: 3.200824022293091\n",
            "4 300 loss: 3.2967264652252197\n",
            "4 acc: 0.2216\n",
            "5 0 loss: 3.291436195373535\n",
            "5 100 loss: 2.960510730743408\n",
            "5 200 loss: 2.8701047897338867\n",
            "5 300 loss: 2.902367353439331\n",
            "5 acc: 0.2487\n",
            "6 0 loss: 2.916325330734253\n",
            "6 100 loss: 3.0213122367858887\n",
            "6 200 loss: 2.7177507877349854\n",
            "6 300 loss: 3.0470547676086426\n",
            "6 acc: 0.2801\n",
            "7 0 loss: 3.0818822383880615\n",
            "7 100 loss: 2.7913033962249756\n",
            "7 200 loss: 2.4345691204071045\n",
            "7 300 loss: 2.7063779830932617\n",
            "7 acc: 0.2877\n",
            "8 0 loss: 2.6217823028564453\n",
            "8 100 loss: 2.728804588317871\n",
            "8 200 loss: 2.6178553104400635\n",
            "8 300 loss: 2.724012851715088\n",
            "8 acc: 0.3239\n",
            "9 0 loss: 2.710331439971924\n",
            "9 100 loss: 2.434113025665283\n",
            "9 200 loss: 2.607243537902832\n",
            "9 300 loss: 2.1172471046447754\n",
            "9 acc: 0.3384\n",
            "10 0 loss: 2.6285736560821533\n",
            "10 100 loss: 2.4359779357910156\n",
            "10 200 loss: 2.0055606365203857\n",
            "10 300 loss: 2.065403461456299\n",
            "10 acc: 0.3471\n",
            "11 0 loss: 2.3644237518310547\n",
            "11 100 loss: 2.197909116744995\n",
            "11 200 loss: 1.8801157474517822\n",
            "11 300 loss: 2.348517417907715\n",
            "11 acc: 0.3577\n",
            "12 0 loss: 2.032717704772949\n",
            "12 100 loss: 2.1547834873199463\n",
            "12 200 loss: 2.0189340114593506\n",
            "12 300 loss: 1.969331979751587\n",
            "12 acc: 0.3638\n",
            "13 0 loss: 1.9548277854919434\n",
            "13 100 loss: 1.7407143115997314\n",
            "13 200 loss: 1.8394098281860352\n",
            "13 300 loss: 1.6087895631790161\n",
            "13 acc: 0.3593\n",
            "14 0 loss: 1.807213544845581\n",
            "14 100 loss: 1.654228925704956\n",
            "14 200 loss: 1.607485055923462\n",
            "14 300 loss: 1.5815811157226562\n",
            "14 acc: 0.3618\n",
            "15 0 loss: 1.5807812213897705\n",
            "15 100 loss: 1.4382542371749878\n",
            "15 200 loss: 1.315314769744873\n",
            "15 300 loss: 1.408247947692871\n",
            "15 acc: 0.3666\n",
            "16 0 loss: 1.2795740365982056\n",
            "16 100 loss: 1.1734826564788818\n",
            "16 200 loss: 0.9274235367774963\n",
            "16 300 loss: 1.0768530368804932\n",
            "16 acc: 0.3611\n",
            "17 0 loss: 1.2886643409729004\n",
            "17 100 loss: 0.7210361361503601\n",
            "17 200 loss: 0.9202716946601868\n",
            "17 300 loss: 1.1177105903625488\n",
            "17 acc: 0.3455\n",
            "18 0 loss: 1.0743553638458252\n",
            "18 100 loss: 1.0139365196228027\n",
            "18 200 loss: 0.5933874249458313\n",
            "18 300 loss: 0.5453739166259766\n",
            "18 acc: 0.3392\n",
            "19 0 loss: 0.8838801383972168\n",
            "19 100 loss: 0.5891622304916382\n",
            "19 200 loss: 0.43700650334358215\n",
            "19 300 loss: 0.5212202072143555\n",
            "19 acc: 0.3487\n",
            "20 0 loss: 0.85239577293396\n",
            "20 100 loss: 0.3726406693458557\n",
            "20 200 loss: 0.4901772439479828\n",
            "20 300 loss: 0.4352439045906067\n",
            "20 acc: 0.3432\n",
            "21 0 loss: 0.5647685527801514\n",
            "21 100 loss: 0.35414496064186096\n",
            "21 200 loss: 0.23219916224479675\n",
            "21 300 loss: 0.40308406949043274\n",
            "21 acc: 0.3469\n",
            "22 0 loss: 0.34765738248825073\n",
            "22 100 loss: 0.3120403289794922\n",
            "22 200 loss: 0.3011976480484009\n",
            "22 300 loss: 0.24370712041854858\n",
            "22 acc: 0.3425\n",
            "23 0 loss: 0.2643880844116211\n",
            "23 100 loss: 0.1284041553735733\n",
            "23 200 loss: 0.17464838922023773\n",
            "23 300 loss: 0.18683606386184692\n",
            "23 acc: 0.3496\n",
            "24 0 loss: 0.2229539155960083\n",
            "24 100 loss: 0.12348797917366028\n",
            "24 200 loss: 0.11529040336608887\n",
            "24 300 loss: 0.1390974223613739\n",
            "24 acc: 0.3458\n",
            "25 0 loss: 0.11867527663707733\n",
            "25 100 loss: 0.2402917593717575\n",
            "25 200 loss: 0.24259981513023376\n",
            "25 300 loss: 0.18955935537815094\n",
            "25 acc: 0.3489\n",
            "26 0 loss: 0.18666377663612366\n",
            "26 100 loss: 0.28458279371261597\n",
            "26 200 loss: 0.14588353037834167\n",
            "26 300 loss: 0.15967142581939697\n",
            "26 acc: 0.3497\n",
            "27 0 loss: 0.34232500195503235\n",
            "27 100 loss: 0.17129787802696228\n",
            "27 200 loss: 0.22383390367031097\n",
            "27 300 loss: 0.09606555104255676\n",
            "27 acc: 0.347\n",
            "28 0 loss: 0.19079230725765228\n",
            "28 100 loss: 0.1604630947113037\n",
            "28 200 loss: 0.27520859241485596\n",
            "28 300 loss: 0.14525330066680908\n",
            "28 acc: 0.3494\n",
            "29 0 loss: 0.1308373510837555\n",
            "29 100 loss: 0.11360369622707367\n",
            "29 200 loss: 0.058495499193668365\n",
            "29 300 loss: 0.08276072889566422\n",
            "29 acc: 0.3417\n",
            "30 0 loss: 0.2645642161369324\n",
            "30 100 loss: 0.09181535243988037\n",
            "30 200 loss: 0.11366100609302521\n",
            "30 300 loss: 0.18712837994098663\n",
            "30 acc: 0.3568\n",
            "31 0 loss: 0.1442185938358307\n",
            "31 100 loss: 0.11467500030994415\n",
            "31 200 loss: 0.16696691513061523\n",
            "31 300 loss: 0.06028331443667412\n",
            "31 acc: 0.3562\n",
            "32 0 loss: 0.08139310032129288\n",
            "32 100 loss: 0.0644860491156578\n",
            "32 200 loss: 0.2586836516857147\n",
            "32 300 loss: 0.15882466733455658\n",
            "32 acc: 0.3553\n",
            "33 0 loss: 0.1436140537261963\n",
            "33 100 loss: 0.12804007530212402\n",
            "33 200 loss: 0.08231103420257568\n",
            "33 300 loss: 0.1152045950293541\n",
            "33 acc: 0.3529\n",
            "34 0 loss: 0.174001544713974\n",
            "34 100 loss: 0.14033973217010498\n",
            "34 200 loss: 0.07787847518920898\n",
            "34 300 loss: 0.13929010927677155\n",
            "34 acc: 0.356\n",
            "35 0 loss: 0.09840193390846252\n",
            "35 100 loss: 0.2049066126346588\n",
            "35 200 loss: 0.2196970283985138\n",
            "35 300 loss: 0.1320093721151352\n",
            "35 acc: 0.3489\n",
            "36 0 loss: 0.06418327987194061\n",
            "36 100 loss: 0.05552011728286743\n",
            "36 200 loss: 0.07441289722919464\n",
            "36 300 loss: 0.1650325059890747\n",
            "36 acc: 0.3545\n",
            "37 0 loss: 0.06404642760753632\n",
            "37 100 loss: 0.1420784443616867\n",
            "37 200 loss: 0.10430175811052322\n",
            "37 300 loss: 0.09361517429351807\n",
            "37 acc: 0.3467\n",
            "38 0 loss: 0.16217109560966492\n",
            "38 100 loss: 0.11309463530778885\n",
            "38 200 loss: 0.11714784801006317\n",
            "38 300 loss: 0.04194638133049011\n",
            "38 acc: 0.3577\n",
            "39 0 loss: 0.10209687799215317\n",
            "39 100 loss: 0.037078455090522766\n",
            "39 200 loss: 0.049349844455718994\n",
            "39 300 loss: 0.07022123038768768\n",
            "39 acc: 0.355\n",
            "40 0 loss: 0.1870163530111313\n",
            "40 100 loss: 0.03565153852105141\n",
            "40 200 loss: 0.05186145007610321\n",
            "40 300 loss: 0.1521216481924057\n",
            "40 acc: 0.3546\n",
            "41 0 loss: 0.07333386689424515\n",
            "41 100 loss: 0.03175841271877289\n",
            "41 200 loss: 0.052457816898822784\n",
            "41 300 loss: 0.1168186143040657\n",
            "41 acc: 0.3485\n",
            "42 0 loss: 0.06853295862674713\n",
            "42 100 loss: 0.0679914802312851\n",
            "42 200 loss: 0.08012250065803528\n",
            "42 300 loss: 0.1219344362616539\n",
            "42 acc: 0.3567\n",
            "43 0 loss: 0.20628702640533447\n",
            "43 100 loss: 0.13998258113861084\n",
            "43 200 loss: 0.04147163778543472\n",
            "43 300 loss: 0.043888360261917114\n",
            "43 acc: 0.3546\n",
            "44 0 loss: 0.14296118915081024\n",
            "44 100 loss: 0.2060830295085907\n",
            "44 200 loss: 0.05856329947710037\n",
            "44 300 loss: 0.09250172972679138\n",
            "44 acc: 0.3528\n",
            "45 0 loss: 0.18395721912384033\n",
            "45 100 loss: 0.12586423754692078\n",
            "45 200 loss: 0.059643201529979706\n",
            "45 300 loss: 0.08180972933769226\n",
            "45 acc: 0.3491\n",
            "46 0 loss: 0.07654964923858643\n",
            "46 100 loss: 0.06724323332309723\n",
            "46 200 loss: 0.08911890536546707\n",
            "46 300 loss: 0.11437129974365234\n",
            "46 acc: 0.3575\n",
            "47 0 loss: 0.1505882889032364\n",
            "47 100 loss: 0.10197723656892776\n",
            "47 200 loss: 0.08391503244638443\n",
            "47 300 loss: 0.0616816021502018\n",
            "47 acc: 0.365\n",
            "48 0 loss: 0.027405373752117157\n",
            "48 100 loss: 0.03630770742893219\n",
            "48 200 loss: 0.08103065192699432\n",
            "48 300 loss: 0.1055925190448761\n",
            "48 acc: 0.3614\n",
            "49 0 loss: 0.043347932398319244\n",
            "49 100 loss: 0.05924433097243309\n",
            "49 200 loss: 0.038807664066553116\n",
            "49 300 loss: 0.04924196004867554\n",
            "49 acc: 0.3565\n"
          ]
        }
      ],
      "source": [
        "import  tensorflow as tf\n",
        "from    tensorflow.keras import layers, optimizers, datasets, Sequential\n",
        "import  os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "tf.random.set_seed(2345)\n",
        "\n",
        "conv_layers = [ # 5 units of conv + max pooling\n",
        "    # unit 1\n",
        "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
        "\n",
        "    # unit 2\n",
        "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
        "\n",
        "    # unit 3\n",
        "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
        "\n",
        "    # unit 4\n",
        "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
        "\n",
        "    # unit 5\n",
        "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
        "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same')\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    # [0~1]\n",
        "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
        "    y = tf.cast(y, dtype=tf.int32)\n",
        "    return x,y\n",
        "\n",
        "\n",
        "(x,y), (x_test, y_test) = datasets.cifar100.load_data()\n",
        "y = tf.squeeze(y, axis=1)\n",
        "y_test = tf.squeeze(y_test, axis=1)\n",
        "print(x.shape, y.shape, x_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "train_db = train_db.shuffle(1000).map(preprocess).batch(128)\n",
        "\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
        "test_db = test_db.map(preprocess).batch(64)\n",
        "\n",
        "sample = next(iter(train_db))\n",
        "print('sample:', sample[0].shape, sample[1].shape,\n",
        "      tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
        "    conv_net = Sequential(conv_layers)\n",
        "\n",
        "    fc_net = Sequential([\n",
        "        layers.Dense(256, activation=tf.nn.relu),\n",
        "        layers.Dense(128, activation=tf.nn.relu),\n",
        "        layers.Dense(100, activation=None),\n",
        "    ])\n",
        "\n",
        "    conv_net.build(input_shape=[None, 32, 32, 3])\n",
        "    fc_net.build(input_shape=[None, 512])\n",
        "    optimizer = optimizers.Adam(lr=1e-4)\n",
        "\n",
        "    # [1, 2] + [3, 4] => [1, 2, 3, 4]\n",
        "    variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
        "\n",
        "    for epoch in range(50):\n",
        "\n",
        "        for step, (x,y) in enumerate(train_db):\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
        "                out = conv_net(x)\n",
        "                # flatten, => [b, 512]\n",
        "                out = tf.reshape(out, [-1, 512])\n",
        "                # [b, 512] => [b, 100]\n",
        "                logits = fc_net(out)\n",
        "                # [b] => [b, 100]\n",
        "                y_onehot = tf.one_hot(y, depth=100)\n",
        "                # compute loss\n",
        "                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
        "                loss = tf.reduce_mean(loss)\n",
        "\n",
        "            grads = tape.gradient(loss, variables)\n",
        "            optimizer.apply_gradients(zip(grads, variables))\n",
        "\n",
        "            if step %100 == 0:\n",
        "                print(epoch, step, 'loss:', float(loss))\n",
        "\n",
        "\n",
        "\n",
        "        total_num = 0\n",
        "        total_correct = 0\n",
        "        for x,y in test_db:\n",
        "\n",
        "            out = conv_net(x)\n",
        "            out = tf.reshape(out, [-1, 512])\n",
        "            logits = fc_net(out)\n",
        "            prob = tf.nn.softmax(logits, axis=1)\n",
        "            pred = tf.argmax(prob, axis=1)\n",
        "            pred = tf.cast(pred, dtype=tf.int32)\n",
        "\n",
        "            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
        "            correct = tf.reduce_sum(correct)\n",
        "\n",
        "            total_num += x.shape[0]\n",
        "            total_correct += int(correct)\n",
        "\n",
        "        acc = total_correct / total_num\n",
        "        print(epoch, 'acc:', acc)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}